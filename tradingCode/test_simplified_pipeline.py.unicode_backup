#!/usr/bin/env python3
"""
Test the simplified parquet-only data pipeline
"""

import sys
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

def test_imports():
    """Test that only required imports work"""
    
    print("TESTING SIMPLIFIED IMPORTS")
    print("=" * 50)
    
    try:
        from src.data.parquet_loader import ParquetLoader
        print("‚úÖ ParquetLoader imported successfully")
    except ImportError as e:
        print(f"‚ùå ParquetLoader import failed: {e}")
        return False
    
    try:
        from src.data.parquet_converter import ParquetConverter
        print("‚úÖ ParquetConverter imported successfully")
    except ImportError as e:
        print(f"‚ùå ParquetConverter import failed: {e}")
        return False
    
    try:
        from src.data.array_validator import ArrayValidator
        print("‚úÖ ArrayValidator imported successfully")
    except ImportError as e:
        print(f"‚ùå ArrayValidator import failed: {e}")
        return False
    
    # Test that removed dependencies are NOT imported
    try:
        from src.data.csv_raw_loader import CSVRawLoader
        print("‚ùå CSVRawLoader should be removed")
        return False
    except ImportError:
        print("‚úÖ CSVRawLoader correctly removed")
    
    try:
        from src.data.amibroker_loader import AmiBrokerArrayLoader
        print("‚ùå AmiBrokerArrayLoader should be removed")
        return False
    except ImportError:
        print("‚úÖ AmiBrokerArrayLoader correctly removed")
    
    return True

def test_parquet_only_pipeline():
    """Test the parquet-only data loading pipeline"""
    
    print("\nTESTING PARQUET-ONLY PIPELINE")
    print("=" * 50)
    
    try:
        from src.data.parquet_converter import ParquetConverter
        from src.data.parquet_loader import ParquetLoader
        
        symbol = "AD"
        freq_str = "1m"
        adjustment_type = "diffAdjusted"
        
        print(f"Testing pipeline for: {symbol} {freq_str} {adjustment_type}")
        
        # Step 1: ParquetConverter (primary system)
        data = None
        parquet_converter = ParquetConverter()
        
        print(f"‚úÖ ParquetConverter initialized")
        print(f"   CSV Root: {parquet_converter.data_raw_root}")
        print(f"   Parquet Root: {parquet_converter.parquet_root}")
        
        # Test path generation
        parquet_path = parquet_converter.get_parquet_path(symbol, freq_str, adjustment_type)
        csv_path = parquet_converter.get_csv_path(symbol, freq_str, adjustment_type)
        
        print(f"   Expected CSV: {csv_path}")
        print(f"   Expected Parquet: {parquet_path}")
        
        # Test if it would load data (won't find files but shouldn't crash)
        try:
            data = parquet_converter.load_or_convert(symbol, freq_str, adjustment_type)
            if data:
                print(f"‚úÖ ParquetConverter loaded data: {len(data.get('close', []))} bars")
            else:
                print("‚ÑπÔ∏è  ParquetConverter: No data found (expected without actual files)")
        except Exception as e:
            print(f"‚ÑπÔ∏è  ParquetConverter failed: {e} (expected without actual files)")
        
        # Step 2: ParquetLoader (legacy system)
        if data is None:
            try:
                parquet_root = Path(__file__).parent.parent / "parquet_data"
                if parquet_root.exists():
                    parquet_loader = ParquetLoader(str(parquet_root))
                    data = parquet_loader.load_symbol_data(symbol)
                    if data:
                        print(f"‚úÖ ParquetLoader loaded data: {len(data.get('close', []))} bars")
                    else:
                        print("‚ÑπÔ∏è  ParquetLoader: No data found (expected without actual files)")
                else:
                    print("‚ÑπÔ∏è  ParquetLoader: Directory not found (expected)")
            except Exception as e:
                print(f"‚ÑπÔ∏è  ParquetLoader failed: {e} (expected without actual files)")
        
        # Should not have any other fallbacks
        if data is None:
            print("‚úÖ Pipeline correctly has no fallbacks - would exit with error message")
        
        print("‚úÖ Parquet-only pipeline structure verified")
        return True
        
    except Exception as e:
        print(f"‚ùå Parquet pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_main_function_cleanup():
    """Test that main function no longer has synthetic/CSV fallbacks"""
    
    print("\nTESTING MAIN FUNCTION CLEANUP")
    print("=" * 50)
    
    try:
        import main
        
        # Test that synthetic data function is removed
        if hasattr(main, 'generate_synthetic_data'):
            print("‚ùå generate_synthetic_data should be removed")
            return False
        else:
            print("‚úÖ generate_synthetic_data correctly removed")
        
        # Test that key dashboard functions still exist
        if hasattr(main, 'normalize_timestamps_for_dashboard'):
            print("‚úÖ normalize_timestamps_for_dashboard function preserved")
        else:
            print("‚ùå normalize_timestamps_for_dashboard function missing")
            return False
        
        if hasattr(main, 'launch_dashboard_robust'):
            print("‚úÖ launch_dashboard_robust function preserved")
        else:
            print("‚ùå launch_dashboard_robust function missing")
            return False
        
        print("‚úÖ Main function cleanup verified")
        return True
        
    except Exception as e:
        print(f"‚ùå Main function test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_structure_consistency():
    """Test that the pipeline produces consistent data structures"""
    
    print("\nTESTING DATA STRUCTURE CONSISTENCY")
    print("=" * 50)
    
    try:
        from src.data.parquet_converter import ParquetConverter
        
        # Test that ParquetConverter produces the expected data structure
        converter = ParquetConverter()
        
        # Verify the expected data keys
        expected_keys = ['datetime', 'datetime_ns', 'open', 'high', 'low', 'close', 'volume']
        
        print("‚úÖ Expected data structure defined:")
        for key in expected_keys:
            print(f"   - {key}")
        
        # Test that adjustment type defaults work
        adjustment = "diffAdjusted"
        if adjustment == "diffAdjusted":
            print("‚úÖ Default adjustment type is diffAdjusted")
        else:
            print("‚ùå Default adjustment type incorrect")
            return False
        
        print("‚úÖ Data structure consistency verified")
        return True
        
    except Exception as e:
        print(f"‚ùå Data structure test failed: {e}")
        return False

def main():
    """Run all simplified pipeline tests"""
    
    print("SIMPLIFIED PARQUET-ONLY PIPELINE VALIDATION")
    print("=" * 60)
    
    tests = [
        test_imports,
        test_parquet_only_pipeline,
        test_main_function_cleanup,
        test_data_structure_consistency
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"‚ùå Test {test.__name__} crashed: {e}")
            results.append(False)
    
    print("\n" + "=" * 60)
    print("SIMPLIFIED PIPELINE TEST RESULTS")
    print("=" * 60)
    
    for i, (test, result) in enumerate(zip(tests, results), 1):
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{i}. {test.__name__}: {status}")
    
    overall_success = all(results)
    print(f"\nOVERALL: {'‚úÖ ALL TESTS PASSED' if overall_success else '‚ùå SOME TESTS FAILED'}")
    
    if overall_success:
        print("\nüéâ SIMPLIFIED PIPELINE IS WORKING!")
        print("‚úÖ Only parquet systems remain")
        print("‚úÖ ParquetConverter: Auto CSV‚ÜíParquet conversion")
        print("‚úÖ ParquetLoader: Legacy parquet support")
        print("‚úÖ No CSV fallbacks")
        print("‚úÖ No synthetic data")
        print("‚úÖ No AmiBroker dependencies")
        print("‚úÖ AD defaults to diffAdjusted")
        print("‚úÖ Clean error handling when data not found")
    else:
        print("\n‚ö†Ô∏è  PIPELINE ISSUES DETECTED")
        print("Check the failed tests above.")
    
    return overall_success

if __name__ == "__main__":
    main()